# -*- coding: utf-8 -*-
"""Phase 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jN-XL-4gAY0Aue-wwu2wi2ONX6WpeJ8a
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')
path = '/content/drive/MyDrive/CIS 412/PS_20174392719_1491204439457_log.csv'

"""# Task
Build, evaluate, and interpret a predictive model to identify fraudulent transactions using the data from "/content/drive/MyDrive/CIS 412/PS_20174392719_1491204439457_log.csv".

## Load Data

### Subtask:
Load the transaction data from the CSV file specified by the 'path' variable into a pandas DataFrame.

**Reasoning**:
Load the CSV file specified by the 'path' variable into a pandas DataFrame named 'df' and display the first few rows.
"""

df = pd.read_csv(path)
df.head()

"""## Explore Data

### Subtask:
Perform initial data exploration to understand the dataset's structure, identify data types, check for missing values, and analyze the distribution of the target variable (fraudulent transactions). This step will help in feature selection and preprocessing decisions.

**Reasoning**:
To begin data exploration, I will display a concise summary of the DataFrame to understand its structure and data types, check for missing values, and display descriptive statistics for numerical columns. Additionally, I will examine the distribution of transaction types and the target variable 'isFraud'.
"""

print("DataFrame Info:")
df.info()

print("\nMissing Values:")
print(df.isnull().sum())

print("\nDescriptive Statistics:")
print(df.describe())

print("\nTransaction Type Distribution:")
print(df['type'].value_counts())

print("\nFraud Distribution:")
print(df['isFraud'].value_counts())

"""## Preprocess Data

### Subtask:
Prepare the data for model training by dropping irrelevant columns, encoding categorical features, and defining features (X) and target (y).

**Reasoning**:
To prepare the data for modeling, I will first drop the 'nameOrig' and 'nameDest' columns as they are unique identifiers and not useful for prediction. Then, I will apply one-hot encoding to the 'type' column to convert categorical values into numerical features suitable for machine learning algorithms. Finally, I will define the feature set 'X' by removing the 'isFraud' column and the target variable 'y' as the 'isFraud' column itself.
"""

df_processed = df.drop(['nameOrig', 'nameDest'], axis=1)
df_processed = pd.get_dummies(df_processed, columns=['type'], drop_first=True)

X = df_processed.drop('isFraud', axis=1)
y = df_processed['isFraud']

print("Shape of X:", X.shape)
print("Shape of y:", y.shape)
print("First 5 rows of X:\n", X.head())
print("First 5 rows of y:\n", y.head())

"""## Split Data and Train Model

### Subtask:
Split the preprocessed data into training and testing sets. Then, select and train a suitable classification model (e.g., Logistic Regression, Decision Tree, or Random Forest) on the training data to predict fraudulent transactions.

**Reasoning**:
To prepare for model training, I will split the preprocessed data into training and testing sets using `train_test_split` with stratification to handle class imbalance. Then, I will initialize and train a `RandomForestClassifier` model on the training data to predict fraudulent transactions.
"""

from sklearn.model_selection import train_test_split
import lightgbm as lgb
import time

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# Initialize the LightGBM Classifier
# Parameters chosen for initial speed, can be tuned later for performance
model = lgb.LGBMClassifier(random_state=42, n_jobs=-1, objective='binary', metric='auc')

# Train the model
print("\nTraining the LightGBM model...")
start_time = time.time()
model.fit(X_train, y_train)
end_time = time.time()
print(f"LightGBM model training complete in {end_time - start_time:.2f} seconds.")

import joblib

# Save the trained model
joblib.dump(model, 'lightgbm_model.pkl')
print("Trained LightGBM model saved as 'lightgbm_model.pkl'")

# Save the feature column names to maintain order during prediction
joblib.dump(X.columns.tolist(), 'feature_columns.pkl')
print("Feature column names saved as 'feature_columns.pkl'")

"""Now that the model and feature columns are saved, here is the code for the Streamlit application. You will need to save this code as a Python file (e.g., `streamlit_app.py`) in the same directory where you saved `random_forest_model.pkl` and `feature_columns.pkl`.

### Instructions to Run the Streamlit App:
1.  **Install Streamlit**: If you don't have it installed, run `pip install streamlit`. Make sure `scikit-learn`, `pandas`, and `joblib` are also installed (`pip install scikit-learn pandas joblib`).
2.  **Save the app**: Copy the Python code below and save it as `streamlit_app.py`.
3.  **Run the app**: Open your terminal or command prompt, navigate to the directory where you saved the files, and run the command: `streamlit run streamlit_app.py`

This will open the Streamlit application in your web browser.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile streamlit_app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import numpy as np
# 
# # Load the trained model and feature columns
# model = joblib.load('lightgbm_model.pkl') # Changed to load lightgbm_model.pkl
# feature_columns = joblib.load('feature_columns.pkl')
# 
# st.title('Fraud Detection App')
# st.write('Enter transaction details to predict if it is fraudulent or not.')
# 
# # Input fields for transaction details
# step = st.number_input('Step (Time step in hours)', min_value=1, value=1)
# amount = st.number_input('Amount', min_value=0.0, value=100.0, format="%.2f")
# oldbalanceOrg = st.number_input('Old Balance Originator', min_value=0.0, value=1000.0, format="%.2f")
# newbalanceOrig = st.number_input('New Balance Originator', min_value=0.0, value=900.0, format="%.2f")
# oldbalanceDest = st.number_input('Old Balance Destination', min_value=0.0, value=500.0, format="%.2f")
# newbalanceDest = st.number_input('New Balance Destination', min_value=0.0, value=600.0, format="%.2f")
# isFlaggedFraud = st.selectbox('Is Flagged Fraud?', [0, 1])
# transaction_type = st.selectbox('Transaction Type', ['CASH_IN', 'PAYMENT', 'CASH_OUT', 'TRANSFER', 'DEBIT'])
# 
# # Prepare input for prediction
# if st.button('Predict Fraud'):
#     # Create a dictionary to hold input values
#     input_data = {
#         'step': step,
#         'amount': amount,
#         'oldbalanceOrg': oldbalanceOrg,
#         'newbalanceOrig': newbalanceOrig,
#         'oldbalanceDest': oldbalanceDest,
#         'newbalanceDest': newbalanceDest,
#         'isFlaggedFraud': isFlaggedFraud,
#     }
# 
#     # Add one-hot encoded 'type' columns, initialized to 0 (integer)
#     for col in ['type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER']:
#         input_data[col] = 0
# 
#     # Set the appropriate 'type' column to 1 based on user selection
#     if transaction_type != 'CASH_IN': # CASH_IN is the base case when drop_first=True, all others are 0
#         encoded_col_name = f'type_{transaction_type}'
#         if encoded_col_name in input_data: # Ensure the column exists
#             input_data[encoded_col_name] = 1
# 
#     # Create a DataFrame from the input data
#     input_df = pd.DataFrame([input_data])
# 
#     # Reorder columns to match the training data
#     # Ensure all feature columns are present, fill missing with 0 if necessary
#     # This also handles cases where a type column might not have been created if drop_first=True
#     for col in feature_columns:
#         if col not in input_df.columns:
#             input_df[col] = 0
#     input_df = input_df[feature_columns]
# 
#     # Make prediction
#     prediction = model.predict(input_df)
#     prediction_proba = model.predict_proba(input_df)
# 
#     st.subheader('Prediction Result:')
#     if prediction[0] == 1:
#         st.error(f"This transaction is likely **Fraudulent** (Probability: {prediction_proba[0][1]:.2f})")
#     else:
#         st.success(f"This transaction is likely **Not Fraudulent** (Probability: {prediction_proba[0][0]:.2f})")

"""# Task
```python
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

# Predict on the test set
y_pred = model.predict(X_test)

# Display classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Visualize confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=True,
            xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Fraud Detection Model')
plt.show()
```

## Evaluate Model Performance

### Subtask:
Evaluate the performance of the trained RandomForestClassifier model using the test set (X_test, y_test). Calculate and display classification report metrics (precision, recall, F1-score) and the confusion matrix to assess the model's ability to identify fraudulent transactions, considering the class imbalance. Additionally, visualize the confusion matrix as a heatmap and include a legend.

**Reasoning**:
To evaluate the model's performance, I will import the necessary metrics and visualization libraries, make predictions on the test set, and then calculate and display the classification report and a heatmap of the confusion matrix.
"""

from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Make predictions on the test set
print("Making predictions on the test set...")
y_pred = model.predict(X_test)
print("Predictions complete.")

# Display classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", conf_matrix)

# Visualize confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=True,
            xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Fraud Detection Model')
plt.show()

"""## Feature Importance Analysis

### Subtask:
Analyze the feature importances from the trained RandomForestClassifier model to identify which features contribute most significantly to the prediction of fraudulent transactions. Visualize these importances using a bar chart for better understanding and provide a legend.

**Reasoning**:
To identify and visualize the most important features, I will extract feature importances from the trained model, create a sorted DataFrame for better handling, and then generate a horizontal bar chart to display them with appropriate labels and title.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Get feature importances from the model
feature_importances = model.feature_importances_

# Create a Series with feature names as index
feature_names = X_train.columns
forest_importances = pd.Series(feature_importances, index=feature_names)

# Sort the feature importances in descending order
forest_importances = forest_importances.sort_values(ascending=True)

# Plot the feature importances
plt.figure(figsize=(10, 8))
forest_importances.plot(kind='barh')
plt.title('Feature Importance for Fraud Detection')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

print("Feature importances calculated and plotted.")

"""## Final Task

### Subtask:
Summarize the performance of the fraud detection model, highlighting key evaluation metrics and the most important features identified for predicting fraudulent transactions.

## Summary:

### Q&A
The fraud detection model exhibits strong performance, particularly in identifying non-fraudulent transactions. For fraudulent transactions (class 1), the model achieved a precision of 0.98, a recall of 0.78, and an F1-score of 0.87. The overall accuracy was 1.00. The most important features for predicting fraudulent transactions were identified and visualized through a bar chart, indicating their relative contributions to the model's predictions.

### Data Analysis Key Findings
*   The model accurately identified 1,906,282 non-fraudulent transactions (True Negatives).
*   Only 40 non-fraudulent transactions were incorrectly classified as fraudulent (False Positives).
*   The model successfully detected 1,920 fraudulent transactions (True Positives).
*   However, 544 actual fraudulent transactions were missed by the model (False Negatives), resulting in a recall of 0.78 for the fraudulent class.
*   The overall accuracy of the model was 1.00.
*   A visualization of feature importances was generated, clearly showing the relative contribution of each feature to the model's fraud prediction capability.

### Insights or Next Steps
*   Given the recall of 0.78 for fraudulent transactions, further investigation into the 544 missed fraudulent transactions (False Negatives) could lead to model improvements, potentially by adjusting thresholds or focusing on features related to these missed cases.
*   The identified important features can be further analyzed to understand the underlying patterns of fraudulent activities, which could inform business rules or preventative measures beyond the model itself.
"""